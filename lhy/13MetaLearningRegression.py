import torch
import torch.nn as nn
import torch.utils.data as data
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
import copy
import matplotlib.pyplot as plt

# ç”Ÿæˆ  ğ‘âˆ—sin(ğ‘¥+ğ‘)  çš„è³‡æ–™é»ï¼Œå…¶ä¸­ ğ‘,ğ‘ çš„ç¯„åœåˆ†åˆ¥é è¨­ç‚º [0.1,5],[0,2ğœ‹] ï¼Œ
# æ¯ä¸€å€‹  ğ‘âˆ—sin(ğ‘¥+ğ‘) çš„å‡½æ•¸æœ‰10å€‹è³‡æ–™é»ç•¶ä½œè¨“ç·´è³‡æ–™ã€‚æ¸¬è©¦æ™‚å‰‡å¯ç”¨è¼ƒå¯†é›†çš„è³‡æ–™é»ç›´æ¥ç”±ç•«åœ–ä¾†çœ‹generalizeçš„å¥½å£ã€‚
# è¿™é‡Œå…¶å®å°±æ˜¯sampleæ•°æ®
device = 'cpu'
def meta_task_data(seed = 0, a_range=[0.1, 5], b_range = [0, 2*np.pi], task_num = 100,
                   n_sample = 10, sample_range = [-5, 5], plot = False):
    np.random.seed = seed
    a_s = np.random.uniform(low = a_range[0], high = a_range[1], size = task_num)  #0.1   5ä»å‡åŒ€åˆ†å¸ƒä¸­å–å€¼ Uniform distribution
    b_s = np.random.uniform(low = b_range[0], high = b_range[1], size = task_num)
    total_x = []
    total_y = []
    label = []
    for t in range(task_num):
        x = np.random.uniform(low = sample_range[0], high = sample_range[1], size = n_sample) # -5 5çš„å‡åŒ€åˆ†å¸ƒä¸­å–å€¼ å–10ä¸ªå€¼
        total_x.append(x)
        total_y.append( a_s[t]*np.sin(x+b_s[t]) )
        label.append('{:.3}*sin(x+{:.3})'.format(a_s[t], b_s[t]))
    if plot:
        plot_x = [np.linspace(-5, 5, 1000)]  #è¿”å›1000ä¸ª ä»-5 åˆ°5ä¹‹é—´çš„å‡åŒ€æ ·æœ¬
        plot_y = []
        for t in range(task_num):
            plot_y.append( a_s[t]*np.sin(plot_x+b_s[t]) )
        return total_x, total_y, plot_x, plot_y, label
    else:
        return total_x, total_y, label


'''
ä»¥ä¸‹æˆ‘å€‘å°‡è€å¸«MAMLæŠ•å½±ç‰‡ç¬¬27é çš„ ğœ™ ç¨±ä½œmeta weight Ï†æ˜¯åˆå§‹åŒ–å‚æ•°ï¼Œ ğœƒ å‰‡ç¨±ç‚ºsub weightã€‚

è€å¸«æŠ•å½±ç‰‡ï¼š http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Meta1%20(v6).pdf

ç‚ºäº†è®“sub weightçš„gradientèƒ½å¤ å‚³åˆ°meta weight (å› ç‚ºsub weightçš„åˆå§‹åŒ–æ˜¯å¾meta weightä¾†çš„ï¼Œæ‰€ä»¥æƒ³ç•¶ç„¶æˆ‘å€‘ç”¨sub weightç®—å‡ºä¾†çš„losså°
meta weightä¹Ÿæ‡‰è©²æ˜¯å¯ä»¥ç®—gradientæ‰å°)ï¼Œé€™é‚Šæˆ‘å€‘éœ€è¦é‡æ–°å®šç¾©ä¸€äº›pytorchå…§çš„layerçš„é‹ç®—ã€‚

å¯¦éš›ä¸ŠMetaLinearé€™å€‹classåšçš„äº‹æƒ…è·Ÿtorch.nn.Linearå®Œå…¨æ˜¯ä¸€æ¨£çš„ï¼Œå”¯ä¸€çš„å·®åˆ¥åœ¨æ–¼é€™é‚Šçš„æ¯ä¸€å€‹tensoréƒ½æ²’æœ‰è¢«è®Šæˆtorch.nn.Parameterã€‚
é€™éº¼åšçš„åŸå› æ˜¯å› ç‚ºç­‰ä¸€ä¸‹æˆ‘å€‘å¾meta weighté‚£è£è¤‡è£½(init weightè¼¸å…¥meta weightå¾Œweightèˆ‡biasä½¿ç”¨.clone)çš„æ™‚å€™ï¼Œtensorçš„cloneçš„æ“ä½œ
æ˜¯å¯ä»¥å‚³égradientçš„ï¼Œä»¥æ–¹ä¾¿æˆ‘å€‘ç”¨gradientæ›´æ–°meta weightã€‚é€™å€‹å¯«æ³•çš„ä»£åƒ¹æ˜¯æˆ‘å€‘å°±æ²’è¾¦æ³•ä½¿ç”¨torch.optimæ›´æ–°sub weightäº†ï¼Œå› ç‚ºåƒæ•¸éƒ½åªç”¨tensorç´€éŒ„ã€‚
ä¹Ÿå› æ­¤æˆ‘å€‘æ¥ä¸‹ä¾†éœ€è¦è‡ªå·±å¯«gradient updateçš„å‡½æ•¸(åªç”¨SGDçš„è©±æ˜¯ç°¡å–®çš„)ã€‚
'''

class MetaLinear(nn.Module):
    '''
    æ‹¿ä¸€æ ·çš„åˆå§‹åŒ–å‚æ•°å»å„ä¸ªtaskä¸Šè®­ç»ƒ
ï¼Œæœ€ç»ˆå­¦å‡ºæ¥çš„modelæ˜¯ä¸ä¸€æ ·çš„
    '''
    def __init__(self, init_layer = None):
        super(MetaLinear, self).__init__()
        if type(init_layer) != type(None):
            self.weight = init_layer.weight.clone()
            self.bias = init_layer.bias.clone()
    def zero_grad(self):
        self.weight.grad  = torch.zeros_like(self.weight)
        self.bias.grad  = torch.zeros_like(self.bias)
    def forward(self, x):
        return F.linear(x, self.weight, self.bias)


'''
é€™è£¡çš„forwardå’Œä¸€èˆ¬çš„modelæ˜¯ä¸€æ¨£çš„ï¼Œå”¯ä¸€çš„å·®åˆ¥åœ¨æ–¼æˆ‘å€‘éœ€è¦å¤šå¯«ä¸€ä¸‹__init__å‡½æ•¸è®“ä»–æ¯”èµ·ä¸€èˆ¬çš„pytorch modelå¤šä¸€å€‹å¯ä»¥å¾meta weight(ğœ™)è¤‡è£½
çš„åŠŸèƒ½(é€™é‚Šå› ç‚ºæˆ‘æŠŠmodelçš„æ¶æ§‹å¯«æ­»äº†æ‰€ä»¥å¯èƒ½çœ‹èµ·ä¾†æœƒæœ‰é»å¤šé¤˜ï¼Œè®€è€…å¯ä»¥è‡ªå·±æŠŠnet()æ”¹æˆå¯ä»¥è‡ªå·±èª¿æ•´æ¶æ§‹çš„æ¨£å­ï¼Œç„¶å¾Œæ€è€ƒä¸€ä¸‹å¦‚ä½•ç”Ÿæˆä¸€å€‹è·Ÿ
meta weightä¸€æ¨£å½¢ç‹€çš„sub weight)

updateå‡½æ•¸å°±å¦‚åŒå‰ä¸€æ®µæåˆ°çš„ï¼Œæˆ‘å€‘éœ€è¦è‡ªå·±å…ˆæ‰‹å‹•ç”¨SGDæ›´æ–°ä¸€æ¬¡sub weightï¼Œæ¥è‘—å†ä½¿ç”¨ä¸‹ä¸€æ­¥çš„gradient(ç¬¬äºŒæ­¥)ä¾†æ›´æ–°
meta weightã€‚zero_gradå‡½æ•¸åœ¨æ­¤è™•æ²’æœ‰ç”¨åˆ°ï¼Œå› ç‚ºå¯¦éš›ä¸Šæˆ‘å€‘è¨ˆç®—ç¬¬äºŒæ­¥çš„gradientæ™‚æœƒéœ€è¦ç¬¬ä¸€æ­¥çš„gradï¼Œé€™ä¹Ÿæ˜¯ç‚ºä»€éº¼æˆ‘å€‘ç¬¬ä¸€æ¬¡backwardçš„æ™‚å€™
éœ€è¦create_graph=True (å»ºç«‹è¨ˆç®—åœ–ä»¥è¨ˆç®—äºŒéšçš„gradient)
'''


class net(nn.Module):
    def __init__(self, init_weight=None):
        super(net, self).__init__()
        if type(init_weight) != type(None):
            for name, module in init_weight.named_modules():
                if name != '':
                    setattr(self, name, MetaLinear(module))
        else:
            self.hidden1 = nn.Linear(1, 40)   #input 1 output 40
            self.hidden2 = nn.Linear(40, 40)   #input 40 output 40
            self.out = nn.Linear(40, 1)      #input 40 output 1

    def zero_grad(self):
        layers = self.__dict__['_modules']
        for layer in layers.keys():
            layers[layer].zero_grad()
    # è¿™é‡Œçš„parentæ˜¯æŒ‡Meta_learning_model  è¿™é‡Œç”¨Meta_learning_modelçš„gradè¿›è¡Œæ›´æ–°
    def update(self, parent, lr=1):
        layers = self.__dict__['_modules']
        parent_layers = parent.__dict__['_modules']
        for param in layers.keys():
            layers[param].weight = layers[param].weight - lr * parent_layers[param].weight.grad
            layers[param].bias = layers[param].bias - lr * parent_layers[param].bias.grad
        # gradient will flow back due to clone backward

    def forward(self, x):
        x = F.relu(self.hidden1(x))
        x = F.relu(self.hidden2(x))
        return self.out(x)

'''
å‰é¢çš„classä¸­æˆ‘å€‘å·²ç¶“éƒ½å°‡è¤‡è£½meta weightåˆ°sub weightï¼Œä»¥åŠsub weightçš„æ›´æ–°ï¼Œgradientçš„å‚³ééƒ½æå®šäº†ï¼Œmeta weightè‡ªå·±æœ¬èº«çš„åƒæ•¸å°±
å¯ä»¥æŒ‰ç…§ä¸€èˆ¬pytorch modelçš„æ¨¡å¼ï¼Œä½¿ç”¨torch.optimä¾†æ›´æ–°äº†ã€‚

gen_modelå‡½æ•¸åšçš„äº‹æƒ…å…¶å¯¦å°±æ˜¯ç”¢ç”ŸNå€‹sub weightï¼Œä¸¦ä¸”ä½¿ç”¨å‰é¢æˆ‘å€‘å¯«å¥½çš„è¤‡è£½meta weightçš„åŠŸèƒ½ã€‚

æ³¨æ„åˆ°è¤‡è£½weightå…¶å¯¦æ˜¯æ•´å€‹codeçš„é—œéµï¼Œå› ç‚ºæˆ‘å€‘éœ€è¦å°‡sub weightè¨ˆç®—çš„ç¬¬äºŒæ­¥gradientæ­£ç¢ºçš„å‚³å›meta weightã€‚è®€è€…å¾meta weightèˆ‡
sub weightæ›´æ–°åƒæ•¸ä½œæ³•çš„å·®åˆ¥(æ‰‹å‹•æ›´æ–°/ç”¨torch.nn.Parameterèˆ‡torch.optim)å¯ä»¥å†æ€è€ƒä¸€ä¸‹å…©è€…çš„å·®åˆ¥ã€‚
'''

class Meta_learning_model():
    def __init__(self, init_weight = None):
        super(Meta_learning_model, self).__init__()
        self.model = net().to(device)
        if type(init_weight) != type(None):
            self.model.load_state_dict(init_weight)
        self.grad_buffer = 0
    def gen_models(self, num, check = True):
        models = [net(init_weight=self.model).to(device) for i in range(num)]  #ç”¨ä¸€å¼€å§‹è®¾ç½®å¥½çš„modelæ¶æ„Linear(1,40)  Linear(40,40) Linear(40,1)ç”Ÿæˆ
        return models
    def clear_buffer(self):
        print("Before grad", self.grad_buffer)
        self.grad_buffer = 0


# æ¥ä¸‹ä¾†å°±æ˜¯ç”Ÿæˆè¨“ç·´/æ¸¬è©¦è³‡æ–™ï¼Œå»ºç«‹meta weightmeta weightçš„æ¨¡å‹ä»¥åŠç”¨ä¾†æ¯”è¼ƒçš„model pretrainingçš„æ¨¡å‹
# batch size 10 ä»£è¡¨æ¯ä¸€è½®æ‰§è¡Œ10ä¸ªä»»åŠ¡
bsz = 10
# æ€»å…±ç”Ÿæˆ 50000*10 ä¸ªä»»åŠ¡ task
train_x, train_y, train_label = meta_task_data(task_num=50000*10)
train_x = torch.Tensor(train_x).unsqueeze(-1) # add one dim ä»ï¼ˆ50000,10) å˜æˆ ï¼ˆ50000,10,1)
train_y = torch.Tensor(train_y).unsqueeze(-1) # y = ğ‘âˆ—sin(ğ‘¥+ğ‘)   ä»ï¼ˆ50000,10) å˜æˆ ï¼ˆ50000,10,1)
# Datasetæ˜¯ä¸€ä¸ªåŒ…è£…ç±»ï¼Œç”¨æ¥å°†æ•°æ®åŒ…è£…ä¸ºDatasetç±»ï¼Œç„¶åä¼ å…¥DataLoaderä¸­ï¼Œæˆ‘ä»¬å†ä½¿ç”¨DataLoaderè¿™ä¸ªç±»æ¥æ›´åŠ å¿«æ·çš„å¯¹æ•°æ®è¿›è¡Œæ“ä½œã€‚
# DataLoaderæ˜¯ä¸€ä¸ªæ¯”è¾ƒé‡è¦çš„ç±»ï¼Œå®ƒä¸ºæˆ‘ä»¬æä¾›çš„å¸¸ç”¨æ“ä½œæœ‰ï¼šbatch_size(æ¯ä¸ªbatchçš„å¤§å°), shuffle(æ˜¯å¦è¿›è¡Œshuffleæ“ä½œ), num_workers(åŠ è½½æ•°æ®çš„æ—¶å€™ä½¿ç”¨å‡ ä¸ªå­è¿›ç¨‹)
train_dataset = data.TensorDataset(train_x, train_y)
train_loader = data.DataLoader(dataset=train_dataset, batch_size=bsz, shuffle=False)

test_x, test_y, plot_x, plot_y, test_label = meta_task_data(task_num=1, n_sample = 10, plot=True)
test_x = torch.Tensor(test_x).unsqueeze(-1) # 1,10,1 add one dim
test_y = torch.Tensor(test_y).unsqueeze(-1) # add one dim
plot_x = torch.Tensor(plot_x).unsqueeze(-1) # add one dim
test_dataset = data.TensorDataset(test_x, test_y)
test_loader = data.DataLoader(dataset=test_dataset, batch_size=bsz, shuffle=False)


meta_model = Meta_learning_model()

meta_optimizer = torch.optim.Adam(meta_model.model.parameters(), lr = 1e-3)


pretrain = net()
pretrain.to(device)
pretrain.train()
pretrain_optim = torch.optim.Adam(pretrain.parameters(), lr = 1e-3)

'''
é€²è¡Œè¨“ç·´ï¼Œæ³¨æ„ä¸€é–‹å§‹æˆ‘å€‘è¦å…ˆç”Ÿæˆä¸€ç¾¤sub weight(codeè£¡é¢çš„sub models)ï¼Œç„¶å¾Œå°‡ä¸€å€‹batchçš„ä¸åŒçš„sinå‡½æ•¸çš„10ç­†è³‡æ–™é»æ‹¿ä¾†è¨“ç·´sub weightã€‚
æ³¨æ„é€™é‚Šsub weightè¨ˆç®—ç¬¬ä¸€æ­¥gradientèˆ‡ç¬¬äºŒæ­¥gradientæ™‚ä½¿ç”¨å„äº”ç­†ä¸é‡è¤‡çš„è³‡æ–™é»(å› æ­¤ä½¿ç”¨[:5]èˆ‡[5:]ä¾†å–)ã€‚ä½†åœ¨è¨“ç·´model pretrainingçš„
å°ç…§çµ„æ™‚å‰‡æ²’æœ‰é€™å€‹å•é¡Œ(æ‰€ä»¥pretrainingçš„modelæ˜¯å¯ä»¥ç¢ºå¯¦çš„èµ°å…©æ­¥gradientçš„)

æ¯ä¸€å€‹sub weightè¨ˆç®—å®Œlosså¾Œç›¸åŠ (å…§å±¤çš„forè¿´åœˆ)å¾Œå°±å¯ä»¥ä½¿ç”¨optimizerä¾†æ›´æ–°meta weightï¼Œå†æ¬¡æé†’ä¸€ä¸‹sub weightè¨ˆç®—ç¬¬ä¸€æ¬¡lossçš„æ™‚å€™
backwardæ˜¯éœ€è¦create_graph=Trueçš„ï¼Œé€™æ¨£è¨ˆç®—ç¬¬äºŒæ­¥gradientçš„æ™‚å€™æ‰æœƒçœŸçš„è¨ˆç®—åˆ°äºŒéšçš„é …ã€‚è®€è€…å¯ä»¥åœ¨é€™å€‹åœ°æ–¹æ€è€ƒä¸€ä¸‹å¦‚ä½•å°‡é€™æ®µç¨‹å¼ç¢¼æ”¹æˆMAMLçš„ä¸€éšåšæ³•ã€‚
'''
epoch = 1
for e in range(epoch):
    meta_model.model.train()
    for x, y in tqdm(train_loader):  #è¿™é‡Œå°±æ˜¯ä¸€è½® ä¸€è½®çš„æ•°æ®é‡å°±æ˜¯batch size å°±æ˜¯ 10
        x = x.to(device)    #è¿™é‡Œçš„xæ˜¯ ã€10,10,1ã€‘   ç¬¬ä¸€ä¸ªæ˜¯batch size 10   ç¬¬äºŒä¸ª 10ä¸ªæ•°æ®ç‚¹å§
        y = y.to(device)    #è¿™é‡Œçš„yæ˜¯ ã€10,10,1ã€‘
        sub_models = meta_model.gen_models(bsz)  #ä¸€é–‹å§‹æˆ‘å€‘è¦å…ˆç”Ÿæˆä¸€ç¾¤ï¼ˆè¿™é‡Œæ˜¯10ä¸ªï¼‰sub weight(codeè£¡é¢çš„sub models)

        meta_l = 0
        for model_num in range(len(sub_models)):
            sample = torch.randint(0, 10, size=(10,), dtype=torch.long)

            # pretraining
            pretrain_optim.zero_grad()
            y_tilde = pretrain(x[model_num][sample[:5], :]) #å–å‡ºæŠ½æ ·ä¸­çš„å‰äº”ä¸ªä¸‹æ ‡å¯¹åº”çš„ ç¬¬model_numä¸ªsub model input xé‡Œçš„å€¼  è¿™é‡Œç®—å‡ºæ¥çš„y_tildeæ˜¯å‰å‘ä¼ æ’­çš„å€¼
            little_l = F.mse_loss(y_tilde, y[model_num][sample[:5], :]) #loss
            little_l.backward()
            pretrain_optim.step()  #ä¼˜åŒ–ä¸€æ­¥ åªæœ‰ç”¨äº†optimizer.step()ï¼Œæ¨¡å‹æ‰ä¼šæ›´æ–°
            pretrain_optim.zero_grad()
            y_tilde = pretrain(x[model_num][sample[5:], :]) #å–å‡ºæŠ½æ ·ä¸­çš„åäº”ä¸ªä¸‹æ ‡å¯¹åº”çš„ ç¬¬model_numä¸ªsub model input xé‡Œçš„å€¼
            little_l = F.mse_loss(y_tilde, y[model_num][sample[5:], :])
            little_l.backward()  #åå‘ä¼ æ’­
            pretrain_optim.step()  #ä¼˜åŒ–ç¬¬äºŒæ­¥ è¿™é‡Œåšåˆ°äºŒé˜¶å¾®åˆ† ç”¨çš„æ˜¯ä¸åŒæ•°æ®

            # meta learning
            # sub weightè¨ˆç®—ç¬¬ä¸€æ­¥gradientèˆ‡ç¬¬äºŒæ­¥gradientæ™‚ä½¿ç”¨å„äº”ç­†ä¸é‡è¤‡çš„è³‡æ–™é»(å› æ­¤ä½¿ç”¨[:5]èˆ‡[5:]ä¾†å–)
            y_tilde = sub_models[model_num](x[model_num][sample[:5], :])   #è¿™é‡Œä¼šè°ƒç”¨å‰é¡¹ä¼ æ’­
            little_l = F.mse_loss(y_tilde, y[model_num][sample[:5], :]) # loss
            # è¨ˆç®—ç¬¬ä¸€æ¬¡gradientä¸¦ä¿ç•™è¨ˆç®—åœ–ä»¥æ¥è‘—è¨ˆç®—æ›´é«˜éšçš„gradient
            little_l.backward(create_graph=True)
            sub_models[model_num].update(lr=1e-2, parent=meta_model.model)  #è‡ªå·±åšæ›´æ–°å‚æ•° è¿™é‡Œä¸ºä»€ä¹ˆè¦è‡ªå·±å†™updateè€Œä¸æ˜¯ç”¨pytorchçš„stepï¼Ÿ
            # å…ˆæ¸…ç©ºoptimizerä¸­è¨ˆç®—çš„gradientå€¼(é¿å…ç´¯åŠ )
            meta_optimizer.zero_grad()

            # è¨ˆç®—ç¬¬äºŒæ¬¡(äºŒéš)çš„gradientï¼ŒäºŒéšçš„åŸå› ä¾†è‡ªç¬¬ä¸€æ¬¡updateæ™‚æœ‰è¨ˆç®—éä¸€æ¬¡gradientäº†
            y_tilde = sub_models[model_num](x[model_num][sample[5:], :])
            meta_l = meta_l + F.mse_loss(y_tilde, y[model_num][sample[5:], :]) #è¿™é‡Œæ˜¯meta learningæ¯ä¸€ä¸ªtaskç´¯åŠ ï¼Ÿ ç›¸å½“äº taskçš„loss

        meta_l = meta_l / bsz
        meta_l.backward() #backwardä»£è¡¨è®¡ç®—gradient
        meta_optimizer.step()
        meta_optimizer.zero_grad()

# æ¸¬è©¦æˆ‘å€‘è¨“ç·´å¥½çš„meta weight
test_model = copy.deepcopy(meta_model.model)
test_model.train()
test_optim = torch.optim.SGD(test_model.parameters(), lr = 1e-3)

# å…ˆç•«å‡ºå¾…æ¸¬è©¦çš„sinå‡½æ•¸ï¼Œä»¥åŠç”¨åœ“é»é»å‡ºæ¸¬è©¦æ™‚çµ¦meta weightè¨“ç·´çš„åç­†è³‡æ–™é»
fig = plt.figure(figsize = [9.6,7.2])
ax = plt.subplot(111)
plot_x1 = plot_x.squeeze().numpy()
ax.scatter(test_x.numpy().squeeze(), test_y.numpy().squeeze())
ax.plot(plot_x1, plot_y[0].squeeze())

# åˆ†åˆ¥åˆ©ç”¨åç­†è³‡æ–™é»æ›´æ–°meta weightä»¥åŠpretrained modelä¸€å€‹step
test_model.train()
pretrain.train()

for epoch in range(1):
    for x, y in test_loader:
        y_tilde = test_model(x[0])
        little_l = F.mse_loss(y_tilde, y[0])
        test_optim.zero_grad()
        little_l.backward()
        test_optim.step()
        print("(meta)))Loss: ", little_l.item())

for epoch in range(1):
    for x, y in test_loader:
        y_tilde = pretrain(x[0])
        little_l = F.mse_loss(y_tilde, y[0])
        pretrain_optim.zero_grad()
        little_l.backward()
        pretrain_optim.step()
        print("(pretrain)Loss: ", little_l.item())

# å°‡æ›´æ–°å¾Œçš„æ¨¡å‹æ‰€ä»£è¡¨çš„å‡½æ•¸ç¹ªè£½å‡ºä¾†ï¼Œèˆ‡çœŸå¯¦çš„sinå‡½æ•¸æ¯”è¼ƒ
test_model.eval()
pretrain.eval()

plot_y_tilde = test_model(plot_x[0]).squeeze().detach().numpy()
plot_x2 = plot_x.squeeze().numpy()
ax.plot(plot_x2, plot_y_tilde, label = 'tune(disjoint)')
ax.legend()
# fig.show()

#
plot_y_tilde = pretrain(plot_x[0]).squeeze().detach().numpy()
plot_x2 = plot_x.squeeze().numpy()
ax.plot(plot_x2, plot_y_tilde, label = 'pretrain')
ax.legend()

# åŸ·è¡Œåº•ä¸‹çš„cellä»¥é¡¯ç¤ºåœ–å½¢ï¼Œä¸¦é‡è¤‡åŸ·è¡Œæ›´æ–°meta weightèˆ‡pretrained modelçš„cellä¾†æ¯”è¼ƒå¤šæ›´æ–°å¹¾æ­¥å¾Œæ˜¯å¦çœŸçš„èƒ½çœ‹å‡ºmeta learningæ¯”model pretrainingæœ‰æ•ˆ
plt.show()





